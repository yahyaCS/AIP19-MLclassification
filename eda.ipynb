{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('used_cars.csv')  \n",
    "df.shape, df.columns\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['price'] = df['price'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "\n",
    "df['milage'] = df['milage'].astype(str).str.replace(r'[^\\d]', '', regex=True)\n",
    "df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
    "\n",
    "df['model_year'] = pd.to_numeric(df['model_year'], errors='coerce')\n",
    "\n",
    "df['engine_l'] = df['engine'].astype(str).str.extract(r'(\\d\\.\\d|\\d)')  # basic\n",
    "df['engine_l'] = pd.to_numeric(df['engine_l'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n",
    "print(df['accident'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['accident_label'] = df['accident'].map({\n",
    "    'None reported': 0,\n",
    "    'At least 1 accident or damage reported': 1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_title_missing'] = df['clean_title'].isna().astype(int)\n",
    "print(\"clean_title missingness vs accident:\\n\", pd.crosstab(df['clean_title_missing'], df['accident_label'], normalize='index'))\n",
    "\n",
    "df['fuel_type_missing'] = df['fuel_type'].isna().astype(int)\n",
    "print(\"\\nfuel_type missingness vs accident:\\n\", pd.crosstab(df['fuel_type_missing'], df['accident_label'], normalize='index'))\n",
    "\n",
    "df['engine_l_missing'] = df['engine_l'].isna().astype(int)\n",
    "print(\"\\nengine_l missingness vs accident:\\n\", pd.crosstab(df['engine_l_missing'], df['accident_label'], normalize='index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows before dropping missing target:\", len(df))\n",
    "df = df[~df['accident_label'].isna()].copy()\n",
    "df['accident_label'] = df['accident_label'].astype(int)\n",
    "print(\"Rows after dropping missing target:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['clean_title', 'fuel_type', 'engine_l']:\n",
    "    flag = col + '_missing'\n",
    "    if flag not in df.columns:\n",
    "        if col in df.columns:\n",
    "            df[flag] = df[col].isna().astype(int)\n",
    "            print(f\"Created {flag}\")\n",
    "        else:\n",
    "            df[flag] = 0\n",
    "            df[col] = np.nan\n",
    "            print(f\"{col} missing â€” created {flag}=0 and {col}=NaN placeholder\")\n",
    "    else:\n",
    "        print(f\"{flag} already exists. Sum(flag)= {df[flag].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['clean_title', 'fuel_type']:\n",
    "    if c in df.columns:\n",
    "        before = df[c].isna().sum()\n",
    "        df[c] = df[c].fillna('Unknown')\n",
    "        after = df[c].isna().sum()\n",
    "        print(f\"{c}: filled {before - after} missing -> 'Unknown'\")\n",
    "\n",
    "if 'engine_l' in df.columns:\n",
    "    median_engine = df['engine_l'].median()\n",
    "    before = df['engine_l'].isna().sum()\n",
    "    df['engine_l'] = df['engine_l'].fillna(median_engine)\n",
    "    after = df['engine_l'].isna().sum()\n",
    "    print(f\"engine_l: filled {before - after} missing -> median {median_engine}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [c for c in ['brand','model','fuel_type','transmission','ext_col','int_col','clean_title'] if c in df.columns]\n",
    "card = {c: df[c].nunique(dropna=False) for c in cat_cols}\n",
    "print(\"Categorical columns and unique counts:\")\n",
    "for k,v in card.items():\n",
    "    print(f\"  - {k}: {v} unique values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_HOT_COLS = ['fuel_type', 'clean_title']                \n",
    "FREQ_COLS = ['brand','model','transmission','ext_col','int_col']  \n",
    "RARE_GROUP_THRESH = 10   \n",
    "DROP_ORIGINALS = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = [c for c in ONE_HOT_COLS + FREQ_COLS if c not in df.columns]\n",
    "if missing_cols:\n",
    "    print(\"Warning: these columns were not found in df and will be skipped:\", missing_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in set(ONE_HOT_COLS + FREQ_COLS):\n",
    "    if c in df.columns:\n",
    "        if df[c].isna().sum() > 0:\n",
    "            df[c] = df[c].fillna('Unknown')\n",
    "            print(f\"Filled NaNs in {c} with 'Unknown' (was present).\")\n",
    "        else:\n",
    "            print(f\"No NaNs in {c} (or already filled).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand has n=57 uniques; skipping rare-grouping\n",
      "Grouped 1830 rare categories into 'Other' for model -> new column model_grp\n",
      "transmission has n=62 uniques; skipping rare-grouping\n",
      "Grouped 298 rare categories into 'Other' for ext_col -> new column ext_col_grp\n",
      "int_col has n=152 uniques; skipping rare-grouping\n"
     ]
    }
   ],
   "source": [
    "for c in FREQ_COLS:\n",
    "    if c in df.columns:\n",
    "        n = df[c].nunique()\n",
    "        if n > 200:  \n",
    "            counts = df[c].value_counts()\n",
    "            rare_vals = counts[counts < RARE_GROUP_THRESH].index\n",
    "            if len(rare_vals) > 0:\n",
    "                newcol = c + '_grp'\n",
    "                if newcol not in df.columns:\n",
    "                    df[newcol] = df[c].where(~df[c].isin(rare_vals), other='Other')\n",
    "                    print(f\"Grouped {len(rare_vals)} rare categories into 'Other' for {c} -> new column {newcol}\")\n",
    "                    df[newcol] = df[newcol].astype(str)\n",
    "                    FREQ_COLS = [newcol if x==c else x for x in FREQ_COLS]\n",
    "                else:\n",
    "                    print(f\"{newcol} already exists; skipping rare-group step for {c}\")\n",
    "            else:\n",
    "                print(f\"No rare categories to group for {c} (n={n})\")\n",
    "        else:\n",
    "            print(f\"{c} has n={n} uniques; skipping rare-grouping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_created = []\n",
    "for col in ONE_HOT_COLS:\n",
    "    if col in df.columns:\n",
    "        prefix = col + '_'\n",
    "        already = any(c.startswith(prefix) for c in df.columns)\n",
    "        if already:\n",
    "            print(f\"One-hot columns for {col} appear to already exist; skipping creation.\")\n",
    "            onehot_created += [c for c in df.columns if c.startswith(prefix)]\n",
    "            continue\n",
    "        dummies = pd.get_dummies(df[col].astype(str), prefix=col, dummy_na=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        created = list(dummies.columns)\n",
    "        onehot_created += created\n",
    "        print(f\"One-hot encoded {col} -> created {len(created)} columns (examples: {created[:4]})\")\n",
    "    else:\n",
    "        print(f\"{col} not in df; skipping one-hot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_created = []\n",
    "for col in FREQ_COLS:\n",
    "    if col in df.columns:\n",
    "        newcol = col + '_freq'\n",
    "        if newcol in df.columns:\n",
    "            print(f\"{newcol} already exists; skipping freq-encoding for {col}.\")\n",
    "            freq_created.append(newcol)\n",
    "            continue\n",
    "        freqs = df[col].value_counts(normalize=True)\n",
    "        df[newcol] = df[col].map(freqs).fillna(0)\n",
    "        freq_created.append(newcol)\n",
    "        print(f\"Frequency-encoded {col} -> {newcol} (sample mapping for top 3: {freqs.head(3).to_dict()})\")\n",
    "    else:\n",
    "        print(f\"{col} not in df; skipping freq-encoding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DROP_ORIGINALS:\n",
    "    originals_to_drop = []\n",
    "    for c in set(ONE_HOT_COLS + [c.replace('_grp','') for c in FREQ_COLS]):  \n",
    "        if c in df.columns:\n",
    "            originals_to_drop.append(c)\n",
    "    if originals_to_drop:\n",
    "        df.drop(columns=originals_to_drop, inplace=True)\n",
    "        print(\"Dropped original text columns:\", originals_to_drop)\n",
    "    else:\n",
    "        print(\"No original text columns found to drop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Verification ===\")\n",
    "print(\"One-hot columns created (sample):\", onehot_created[:10])\n",
    "print(\"Freq columns created (sample):\", freq_created[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_cols = (onehot_created + freq_created + ['clean_title_missing','fuel_type_missing','engine_l_missing'])\n",
    "preview_cols = [c for c in preview_cols if c in df.columns][:20]\n",
    "print(\"\\nPreview (first 5 rows) of encoded columns:\")\n",
    "display(df[preview_cols].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe shape after encoding:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "numeric_features = [c for c in ['price','milage','age','engine_l','model_year'] if c in df.columns]\n",
    "\n",
    "freq_cols = [c for c in df.columns if c.endswith('_freq')]\n",
    "\n",
    "onehot_cols = [c for c in df.columns if c.startswith('fuel_type_') or c.startswith('clean_title_')]\n",
    "\n",
    "missing_flags = [c for c in ['clean_title_missing','fuel_type_missing','engine_l_missing'] if c in df.columns]\n",
    "\n",
    "final_features = numeric_features + freq_cols + onehot_cols + missing_flags\n",
    "\n",
    "final_features = [f for i,f in enumerate(final_features) if f not in final_features[:i]]\n",
    "\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "print(\"Frequency-encoded features:\", freq_cols)\n",
    "print(\"One-hot features (detected):\", onehot_cols[:10])\n",
    "print(\"Missingness flags:\", missing_flags)\n",
    "print(\"\\nNumber of final features:\", len(final_features))\n",
    "print(\"Sample final features (first 60):\", final_features[:60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numeric features summary:\")\n",
    "display(df[numeric_features].describe().T[['count','mean','50%','std']])\n",
    "\n",
    "print(\"\\nTop values / frequencies for freq-encoded columns (top 5):\")\n",
    "for c in freq_cols:\n",
    "    print(f\"\\n{c} top 5 value->freq:\")\n",
    "    original = c.replace('_freq','')\n",
    "    if original in df.columns:\n",
    "        print(df[original].value_counts().head(5))\n",
    "    print(df[c].value_counts().head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[final_features].fillna(0).copy()   \n",
    "y = df['accident_label'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape :\", X_test.shape)\n",
    "print(\"\\nTrain label distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTrain label distribution (proportions):\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest label distribution (proportions):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = neg / pos if pos != 0 else 1.0\n",
    "print(f\"\\nTraining negatives: {neg}, positives: {pos}\")\n",
    "print(f\"Computed scale_pos_weight (neg/pos): {scale_pos_weight:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat([y_train, X_train], axis=1)\n",
    "test_full = pd.concat([y_test, X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.to_csv(\"usedcars_xgb_train.csv\", header=False, index=False)\n",
    "test_full.to_csv(\"usedcars_xgb_test.csv\", header=False, index=False)\n",
    "\n",
    "print(\"Saved files:\")\n",
    "!ls -lh usedcars_xgb_train.csv usedcars_xgb_test.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
