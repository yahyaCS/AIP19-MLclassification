{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('used_cars.csv')  \n",
    "df.shape, df.columns\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['price'] = df['price'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
    "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "\n",
    "df['milage'] = df['milage'].astype(str).str.replace(r'[^\\d]', '', regex=True)\n",
    "df['milage'] = pd.to_numeric(df['milage'], errors='coerce')\n",
    "\n",
    "df['model_year'] = pd.to_numeric(df['model_year'], errors='coerce')\n",
    "\n",
    "df['engine_l'] = df['engine'].astype(str).str.extract(r'(\\d\\.\\d|\\d)')  # basic\n",
    "df['engine_l'] = pd.to_numeric(df['engine_l'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n",
    "print(df['accident'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['accident_label'] = df['accident'].map({\n",
    "    'None reported': 0,\n",
    "    'At least 1 accident or damage reported': 1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_title_missing'] = df['clean_title'].isna().astype(int)\n",
    "print(\"clean_title missingness vs accident:\\n\", pd.crosstab(df['clean_title_missing'], df['accident_label'], normalize='index'))\n",
    "\n",
    "df['fuel_type_missing'] = df['fuel_type'].isna().astype(int)\n",
    "print(\"\\nfuel_type missingness vs accident:\\n\", pd.crosstab(df['fuel_type_missing'], df['accident_label'], normalize='index'))\n",
    "\n",
    "df['engine_l_missing'] = df['engine_l'].isna().astype(int)\n",
    "print(\"\\nengine_l missingness vs accident:\\n\", pd.crosstab(df['engine_l_missing'], df['accident_label'], normalize='index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows before dropping missing target:\", len(df))\n",
    "df = df[~df['accident_label'].isna()].copy()\n",
    "df['accident_label'] = df['accident_label'].astype(int)\n",
    "print(\"Rows after dropping missing target:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['clean_title', 'fuel_type', 'engine_l']:\n",
    "    flag = col + '_missing'\n",
    "    if flag not in df.columns:\n",
    "        if col in df.columns:\n",
    "            df[flag] = df[col].isna().astype(int)\n",
    "            print(f\"Created {flag}\")\n",
    "        else:\n",
    "            df[flag] = 0\n",
    "            df[col] = np.nan\n",
    "            print(f\"{col} missing â€” created {flag}=0 and {col}=NaN placeholder\")\n",
    "    else:\n",
    "        print(f\"{flag} already exists. Sum(flag)= {df[flag].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['clean_title', 'fuel_type']:\n",
    "    if c in df.columns:\n",
    "        before = df[c].isna().sum()\n",
    "        df[c] = df[c].fillna('Unknown')\n",
    "        after = df[c].isna().sum()\n",
    "        print(f\"{c}: filled {before - after} missing -> 'Unknown'\")\n",
    "\n",
    "if 'engine_l' in df.columns:\n",
    "    median_engine = df['engine_l'].median()\n",
    "    before = df['engine_l'].isna().sum()\n",
    "    df['engine_l'] = df['engine_l'].fillna(median_engine)\n",
    "    after = df['engine_l'].isna().sum()\n",
    "    print(f\"engine_l: filled {before - after} missing -> median {median_engine}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [c for c in ['brand','model','fuel_type','transmission','ext_col','int_col','clean_title'] if c in df.columns]\n",
    "card = {c: df[c].nunique(dropna=False) for c in cat_cols}\n",
    "print(\"Categorical columns and unique counts:\")\n",
    "for k,v in card.items():\n",
    "    print(f\"  - {k}: {v} unique values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_HOT_COLS = ['fuel_type', 'clean_title']                \n",
    "FREQ_COLS = ['brand','model','transmission','ext_col','int_col']  \n",
    "RARE_GROUP_THRESH = 10   \n",
    "DROP_ORIGINALS = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = [c for c in ONE_HOT_COLS + FREQ_COLS if c not in df.columns]\n",
    "if missing_cols:\n",
    "    print(\"Warning: these columns were not found in df and will be skipped:\", missing_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in set(ONE_HOT_COLS + FREQ_COLS):\n",
    "    if c in df.columns:\n",
    "        if df[c].isna().sum() > 0:\n",
    "            df[c] = df[c].fillna('Unknown')\n",
    "            print(f\"Filled NaNs in {c} with 'Unknown' (was present).\")\n",
    "        else:\n",
    "            print(f\"No NaNs in {c} (or already filled).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand has n=57 uniques; skipping rare-grouping\n",
      "Grouped 1830 rare categories into 'Other' for model -> new column model_grp\n",
      "transmission has n=62 uniques; skipping rare-grouping\n",
      "Grouped 298 rare categories into 'Other' for ext_col -> new column ext_col_grp\n",
      "int_col has n=152 uniques; skipping rare-grouping\n"
     ]
    }
   ],
   "source": [
    "for c in FREQ_COLS:\n",
    "    if c in df.columns:\n",
    "        n = df[c].nunique()\n",
    "        if n > 200:  \n",
    "            counts = df[c].value_counts()\n",
    "            rare_vals = counts[counts < RARE_GROUP_THRESH].index\n",
    "            if len(rare_vals) > 0:\n",
    "                newcol = c + '_grp'\n",
    "                if newcol not in df.columns:\n",
    "                    df[newcol] = df[c].where(~df[c].isin(rare_vals), other='Other')\n",
    "                    print(f\"Grouped {len(rare_vals)} rare categories into 'Other' for {c} -> new column {newcol}\")\n",
    "                    df[newcol] = df[newcol].astype(str)\n",
    "                    FREQ_COLS = [newcol if x==c else x for x in FREQ_COLS]\n",
    "                else:\n",
    "                    print(f\"{newcol} already exists; skipping rare-group step for {c}\")\n",
    "            else:\n",
    "                print(f\"No rare categories to group for {c} (n={n})\")\n",
    "        else:\n",
    "            print(f\"{c} has n={n} uniques; skipping rare-grouping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_created = []\n",
    "for col in ONE_HOT_COLS:\n",
    "    if col in df.columns:\n",
    "        prefix = col + '_'\n",
    "        already = any(c.startswith(prefix) for c in df.columns)\n",
    "        if already:\n",
    "            print(f\"One-hot columns for {col} appear to already exist; skipping creation.\")\n",
    "            onehot_created += [c for c in df.columns if c.startswith(prefix)]\n",
    "            continue\n",
    "        dummies = pd.get_dummies(df[col].astype(str), prefix=col, dummy_na=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        created = list(dummies.columns)\n",
    "        onehot_created += created\n",
    "        print(f\"One-hot encoded {col} -> created {len(created)} columns (examples: {created[:4]})\")\n",
    "    else:\n",
    "        print(f\"{col} not in df; skipping one-hot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_created = []\n",
    "for col in FREQ_COLS:\n",
    "    if col in df.columns:\n",
    "        newcol = col + '_freq'\n",
    "        if newcol in df.columns:\n",
    "            print(f\"{newcol} already exists; skipping freq-encoding for {col}.\")\n",
    "            freq_created.append(newcol)\n",
    "            continue\n",
    "        freqs = df[col].value_counts(normalize=True)\n",
    "        df[newcol] = df[col].map(freqs).fillna(0)\n",
    "        freq_created.append(newcol)\n",
    "        print(f\"Frequency-encoded {col} -> {newcol} (sample mapping for top 3: {freqs.head(3).to_dict()})\")\n",
    "    else:\n",
    "        print(f\"{col} not in df; skipping freq-encoding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DROP_ORIGINALS:\n",
    "    originals_to_drop = []\n",
    "    for c in set(ONE_HOT_COLS + [c.replace('_grp','') for c in FREQ_COLS]):  \n",
    "        if c in df.columns:\n",
    "            originals_to_drop.append(c)\n",
    "    if originals_to_drop:\n",
    "        df.drop(columns=originals_to_drop, inplace=True)\n",
    "        print(\"Dropped original text columns:\", originals_to_drop)\n",
    "    else:\n",
    "        print(\"No original text columns found to drop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Verification ===\")\n",
    "print(\"One-hot columns created (sample):\", onehot_created[:10])\n",
    "print(\"Freq columns created (sample):\", freq_created[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_cols = (onehot_created + freq_created + ['clean_title_missing','fuel_type_missing','engine_l_missing'])\n",
    "preview_cols = [c for c in preview_cols if c in df.columns][:20]\n",
    "print(\"\\nPreview (first 5 rows) of encoded columns:\")\n",
    "display(df[preview_cols].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe shape after encoding:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "numeric_features = [c for c in ['price','milage','age','engine_l','model_year'] if c in df.columns]\n",
    "\n",
    "freq_cols = [c for c in df.columns if c.endswith('_freq')]\n",
    "\n",
    "onehot_cols = [c for c in df.columns if c.startswith('fuel_type_') or c.startswith('clean_title_')]\n",
    "\n",
    "missing_flags = [c for c in ['clean_title_missing','fuel_type_missing','engine_l_missing'] if c in df.columns]\n",
    "\n",
    "final_features = numeric_features + freq_cols + onehot_cols + missing_flags\n",
    "\n",
    "final_features = [f for i,f in enumerate(final_features) if f not in final_features[:i]]\n",
    "\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "print(\"Frequency-encoded features:\", freq_cols)\n",
    "print(\"One-hot features (detected):\", onehot_cols[:10])\n",
    "print(\"Missingness flags:\", missing_flags)\n",
    "print(\"\\nNumber of final features:\", len(final_features))\n",
    "print(\"Sample final features (first 60):\", final_features[:60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numeric features summary:\")\n",
    "display(df[numeric_features].describe().T[['count','mean','50%','std']])\n",
    "\n",
    "print(\"\\nTop values / frequencies for freq-encoded columns (top 5):\")\n",
    "for c in freq_cols:\n",
    "    print(f\"\\n{c} top 5 value->freq:\")\n",
    "    original = c.replace('_freq','')\n",
    "    if original in df.columns:\n",
    "        print(df[original].value_counts().head(5))\n",
    "    print(df[c].value_counts().head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[final_features].fillna(0).copy()   \n",
    "y = df['accident_label'].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape :\", X_test.shape)\n",
    "print(\"\\nTrain label distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTrain label distribution (proportions):\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest label distribution (proportions):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "neg = int((y_train == 0).sum())\n",
    "pos = int((y_train == 1).sum())\n",
    "scale_pos_weight = neg / pos if pos != 0 else 1.0\n",
    "print(f\"\\nTraining negatives: {neg}, positives: {pos}\")\n",
    "print(f\"Computed scale_pos_weight (neg/pos): {scale_pos_weight:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat([y_train, X_train], axis=1)\n",
    "test_full = pd.concat([y_test, X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.to_csv(\"usedcars_xgb_train.csv\", header=False, index=False)\n",
    "test_full.to_csv(\"usedcars_xgb_test.csv\", header=False, index=False)\n",
    "\n",
    "print(\"Saved files:\")\n",
    "!ls -lh usedcars_xgb_train.csv usedcars_xgb_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_patterns = [\"usedcars_xgb_train.csv\"]\n",
    "test_patterns  = [\"usedcars_xgb_test.csv\"]\n",
    "\n",
    "def find_first(patterns):\n",
    "    files = []\n",
    "    for p in patterns:\n",
    "        files += glob.glob(p)\n",
    "    files = sorted(list(dict.fromkeys(files)))\n",
    "    return files\n",
    "\n",
    "train_files = find_first(train_patterns)\n",
    "test_files  = find_first(test_patterns)\n",
    "print(\"Train candidates:\", train_files)\n",
    "print(\"Test candidates: \", test_files)\n",
    "if not train_files or not test_files:\n",
    "    raise FileNotFoundError(\"No train/test CSVs found in working dir. Rename or place them here. Patterns used: train_patterns/test_patterns\")\n",
    "\n",
    "train_path = train_files[0]\n",
    "test_path  = test_files[0]\n",
    "print(\"Using files:\", train_path, test_path)\n",
    "\n",
    "def read_detect(path):\n",
    "    df0 = pd.read_csv(path, header=None)\n",
    "    col0 = pd.to_numeric(df0.iloc[:,0], errors='coerce')\n",
    "    frac_binary = ((col0==0) | (col0==1)).sum() / float(len(col0))\n",
    "    if col0.notna().all() and frac_binary >= 0.9:\n",
    "        ncols = df0.shape[1]\n",
    "        cols = ['accident_label'] + [f'feat_{i}' for i in range(1,ncols)]\n",
    "        df0.columns = cols\n",
    "        return df0, 'sagemaker_no_header'\n",
    "    dfh = pd.read_csv(path, header=0)\n",
    "    # find label-like column\n",
    "    lcands = [c for c in dfh.columns if c.lower() in ('accident_label','accident','label','target','y')]\n",
    "    if lcands:\n",
    "        dfh = dfh.rename(columns={lcands[0]:'accident_label'})\n",
    "        return dfh, 'header_with_label'\n",
    "    # try infer first column numeric 0/1\n",
    "    col0h = pd.to_numeric(dfh.iloc[:,0], errors='coerce')\n",
    "    if col0h.notna().all() and set(col0h.unique()).issubset({0.0,1.0}):\n",
    "        dfh = dfh.rename(columns={dfh.columns[0]:'accident_label'})\n",
    "        return dfh, 'header_inferred_label'\n",
    "    return dfh, 'unknown'\n",
    "\n",
    "train_df_raw, train_type = read_detect(train_path)\n",
    "test_df_raw,  test_type  = read_detect(test_path)\n",
    "print(\"Train type:\", train_type, \"shape:\", train_df_raw.shape)\n",
    "print(\"Test  type:\", test_type,  \"shape:\", test_df_raw.shape)\n",
    "\n",
    "if train_type=='unknown' or test_type=='unknown':\n",
    "    print(\"Train head:\\n\", train_df_raw.head().to_string())\n",
    "    print(\"Test  head:\\n\", test_df_raw.head().to_string())\n",
    "    raise RuntimeError(\"Could not auto-detect label column. Ensure label column is named 'accident_label' or files are SageMaker-style (label first, no header).\")\n",
    "\n",
    "# ensure label numeric and available\n",
    "for name, df in (('train',train_df_raw),('test',test_df_raw)):\n",
    "    if 'accident_label' not in df.columns:\n",
    "        raise RuntimeError(f\"accident_label missing in {name}\")\n",
    "    df['accident_label'] = pd.to_numeric(df['accident_label'], errors='coerce').astype('Int64')\n",
    "    if df['accident_label'].isna().any():\n",
    "        raise RuntimeError(f\"Some labels in {name} could not be converted to numeric 0/1\")\n",
    "\n",
    "# choose common numeric features between train and test\n",
    "def numeric_cols(df):\n",
    "    cols = [c for c in df.columns if c!='accident_label']\n",
    "    num = []\n",
    "    for c in cols:\n",
    "        s = pd.to_numeric(df[c], errors='coerce')\n",
    "        if s.notna().sum() > 0: num.append(c)\n",
    "    return num\n",
    "\n",
    "train_num = numeric_cols(train_df_raw)\n",
    "test_num  = numeric_cols(test_df_raw)\n",
    "common = [c for c in train_num if c in test_num]\n",
    "if not common:\n",
    "    # fallback to positional features if headerless sage-maker format\n",
    "    common = [c for c in train_df_raw.columns if c!='accident_label']\n",
    "print(\"Using\", len(common), \"common numeric features:\", common[:10])\n",
    "\n",
    "train_df = train_df_raw[['accident_label']+common].copy()\n",
    "test_df  = test_df_raw[['accident_label']+common].copy()\n",
    "combined = pd.concat([train_df, test_df], ignore_index=True)\n",
    "print(\"Combined shape:\", combined.shape)\n",
    "display(combined.head())\n",
    "\n",
    "# stratified split 70/15/15\n",
    "X = combined.drop(columns=['accident_label']).copy()\n",
    "y = combined['accident_label'].astype(int).copy()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(\" X_train:\", X_train.shape, \"y_train:\", y_train.value_counts().to_dict())\n",
    "print(\" X_val:  \", X_val.shape,   \"y_val:  \", y_val.value_counts().to_dict())\n",
    "print(\" X_test: \", X_test.shape,  \"y_test: \", y_test.value_counts().to_dict())\n",
    "\n",
    "# compute scale_pos_weight\n",
    "neg = int((y_train==0).sum()); pos = int((y_train==1).sum())\n",
    "scale_pos_weight = neg/pos if pos!=0 else 1.0\n",
    "print(f\"\\nscale_pos_weight (neg/pos) computed on training set: {scale_pos_weight:.6f} (neg={neg}, pos={pos})\")\n",
    "\n",
    "# Save SageMaker-style CSVs (label first, no header)\n",
    "def save_sm(Xdf, ydf, fname):\n",
    "    arr = np.hstack([ydf.values.reshape(-1,1), Xdf.values])\n",
    "    pd.DataFrame(arr).to_csv(fname, index=False, header=False)\n",
    "\n",
    "out_train = \"usedcars_xgb_train_resplit.csv\"\n",
    "out_val   = \"usedcars_xgb_val_resplit.csv\"\n",
    "out_test  = \"usedcars_xgb_test_resplit.csv\"\n",
    "save_sm(X_train, y_train, out_train)\n",
    "save_sm(X_val,   y_val,   out_val)\n",
    "save_sm(X_test,  y_test,  out_test)\n",
    "print(\"\\nSaved files:\")\n",
    "!ls -lh usedcars_xgb_*resplit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features-only file: usedcars_xgb_test_features.csv\n"
     ]
    }
   ],
   "source": [
    "test_with_label = 'usedcars_xgb_test_resplit.csv'   \n",
    "test_features_only = 'usedcars_xgb_test_features.csv'\n",
    "\n",
    "# Load and drop first column (label)\n",
    "df = pd.read_csv(test_with_label, header=None)\n",
    "# Drop column 0 (label), save features only as CSV (no header, no index)\n",
    "df.iloc[:, 1:].to_csv(test_features_only, index=False, header=False)\n",
    "print(\"Saved features-only file:\", test_features_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tar_path = \"model.tar.gz\"                     \n",
    "extracted_model_dir = \"model_extracted\"             \n",
    "test_csv_path = \"usedcars_xgb_test_resplit.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tarfile, glob\n",
    "import pandas as pd, numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_tar_path and os.path.exists(model_tar_path):\n",
    "    os.makedirs(extracted_model_dir, exist_ok=True)\n",
    "    with tarfile.open(model_tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extracted_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model file: model_extracted/xgboost-model\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for root, dirs, files in os.walk(extracted_model_dir):\n",
    "    for f in files:\n",
    "        lf = f.lower()\n",
    "        if lf.startswith(\"xgboost\") or lf.endswith(\".model\") or lf.endswith(\".bin\") or lf.endswith(\".bst\") or lf.endswith(\".json\"):\n",
    "            candidates.append(os.path.join(root, f))\n",
    "if not candidates:\n",
    "    candidates = [os.path.join(extracted_model_dir, f) for f in os.listdir(extracted_model_dir) if os.path.isfile(os.path.join(extracted_model_dir, f))]\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(f\"No model file found under {extracted_model_dir}. Files: {os.listdir(extracted_model_dir)}\")\n",
    "model_file = candidates[0]\n",
    "print(\"Using model file:\", model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.Booster()\n",
    "bst.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_csv(path):\n",
    "    df_try = pd.read_csv(path, header=None)\n",
    "    col0 = pd.to_numeric(df_try.iloc[:,0], errors='coerce')\n",
    "    frac_binary = ((col0==0) | (col0==1)).sum() / float(len(col0))\n",
    "    if col0.notna().all() and frac_binary >= 0.9:\n",
    "        df_try.columns = ['accident_label'] + [f'feat_{i}' for i in range(1, df_try.shape[1])]\n",
    "        return df_try\n",
    "    df_h = pd.read_csv(path, header=0)\n",
    "    label_candidates = [c for c in df_h.columns if c.lower() in ('accident_label','accident','label','target','y')]\n",
    "    if label_candidates:\n",
    "        df_h = df_h.rename(columns={label_candidates[0]: 'accident_label'})\n",
    "        return df_h\n",
    "    col0h = pd.to_numeric(df_h.iloc[:,0], errors='coerce')\n",
    "    if col0h.notna().all() and set(col0h.unique()).issubset({0.0,1.0}):\n",
    "        df_h = df_h.rename(columns={df_h.columns[0]:'accident_label'})\n",
    "        return df_h\n",
    "    raise RuntimeError(\"Could not detect label column in test CSV.\")\n",
    "\n",
    "df_test = load_test_csv(test_csv_path)\n",
    "df_test['accident_label'] = pd.to_numeric(df_test['accident_label'], errors='coerce').astype(int)\n",
    "y_test = df_test.iloc[:,0].values\n",
    "X_test = df_test.iloc[:,1:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions_thresh_0.3471.csv\n",
      "Predicted positives: 340 / 585 (58.120%)\n",
      "Expected alerts per 1000 samples: 581.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "threshold = 0.3471\n",
    "preds = (probs >= threshold).astype(int)\n",
    "\n",
    "# save predictions with probs and true labels\n",
    "res = pd.DataFrame({'prob_pos': probs, 'pred': preds, 'label': y_test})\n",
    "res.to_csv('predictions_thresh_0.3471.csv', index=False)\n",
    "print(\"Saved predictions_thresh_0.3471.csv\")\n",
    "\n",
    "# compute expected alerts per 1000\n",
    "prop_pos = preds.mean()\n",
    "print(f\"Predicted positives: {preds.sum()} / {len(preds)} ({prop_pos:.3%})\")\n",
    "print(f\"Expected alerts per 1000 samples: {prop_pos*1000:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.701403921083555\n",
      "Precision: 0.3500, Recall: 0.8041, F1: 0.4877\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.49      0.63       437\n",
      "           1       0.35      0.80      0.49       148\n",
      "\n",
      "    accuracy                           0.57       585\n",
      "   macro avg       0.62      0.65      0.56       585\n",
      "weighted avg       0.75      0.57      0.60       585\n",
      "\n",
      "Confusion matrix:\n",
      "[[216 221]\n",
      " [ 29 119]]\n"
     ]
    }
   ],
   "source": [
    "# Convert to DMatrix for XGBoost\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Predict probabilities\n",
    "y_proba = bst.predict(dtest)\n",
    "\n",
    "\n",
    "y_pred = (y_proba >= 0.3471).astype(int)\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(\"Test AUC:\", auc)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgbenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
